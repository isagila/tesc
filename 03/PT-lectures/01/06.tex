\subsection{%
  Лекция \texttt{23.10.10}.%
}

\subheader{Случайные величины}

\begin{definition}
  Пусть имеется вероятностное пространство \(\tuple{\Omega, \euF, \probP}\),
  функция \(\xi \colon \Omega \to \RR\) называется \(\euF\)-измеримой, если

  \begin{equation*}
    \forall x \in \RR \given \set{ \omega \in \Omega \given \xi(\omega) < x} \in
    \euF
  \end{equation*}
  
  Таким образом \(\xi^{-1}(\interval{-\infty}{x}) \in \euF\).
\end{definition}

\begin{definition}
  Случайной величиной, заданной на вероятностном пространстве \(\tuple{\Omega,
  \euF, \probP}\) называется \(\euF\)-измеримая функция \(xi \colon \Omega \to
  \RR\), сопоставляющая каждому элементарному исходу некоторое вещественное
  число.
\end{definition}

\begin{remark}
  Не все функции являются измеримыми. Например, рассмотрим бросок кости,
  \(\Omega = \set{1, 2, 3, 4, 5, 6}\), \(\euF = \set{\varnothing, \Omega, A =
  \set{2, 4, 6}, \bar{A} = \set{1, 3, 5}}\). Если \(\xi(x) = i\), то такая
  функция не будет \(\euF\)-измеримой, потому что при \(x = 4\) получается

  \begin{equation*}
    \set{ \omega \in \Omega \given \xi(\omega) < x}
    = \set{1, 2, 3} \notin \euF
  \end{equation*}

  Если же определить \(\xi(x)\) как \(\xi(2) = \xi(4) = \xi(6) = 1\), \(\xi(1) =
  \xi(3) = \xi(5) = 0\), то \(\xi(x)\) будет \(\euF\)-измерима.
\end{remark}

\subheader{Смысл измеримости}

Если задана случайная величина \(\xi\), то мы можем задать вероятность попадания
случайной величины в интервал \(\interval{-\infty}{x}\), получаем \(\prob{\xi
\in \interval{-\infty}{x}} = \prob{\omega \in \Omega \given \xi(\omega) < x}\).
C помощью операций объединения, пересечения и дополнения из этих интервалов мы
можем получить любые другие интервалы (включая точки) и также приписать им
вероятности. Далее согласно теореме Каратеодори мы можем данную вероятностную
меру однозначно продолжить на всю борелевскую \(\sigma\)-алгебру вещественной
прямой и таким образом \(\forall B \in \mathcal{B} (\RR)\) будет определена
вероятность \(\prob{B} = \prob{\set{\omega \in \Omega \given \xi(\omega) \in
B}}\).

Итак, пусть \(\xi\)~--- случайная величина, заданная на вероятностном
пространстве \(\tuple{\Omega, \euF, \probP}\). Тогда получаем новое
вероятностное пространство:

\begin{equation*}
  \tuple{\Omega, \euF, \probP} \Rarr{\xi} \tuple{\RR, \mathcal{B}, \probP_{\xi}}
\end{equation*}

Полученное вероятностное пространство называется индуцированным с помощью
случайной величины \(\xi\).

\begin{definition}
  Распределением называется функция \(\prob{B}\), сопоставляющая каждому
  борелевскому множеству на прямой вероятность. Это называется распределением
  вероятностей случайной величины \(\xi(\omega)\).
\end{definition}

Распределения бывают дискретными и абсолютно непрерывными.

\subheader{Дискретные случайные величины}

\begin{definition}
  Случайная величина \(\xi\) имеет дискретное распределения, если она принимает
  не более, чем счетное число значений.

  \begin{equation*}
    \exists \set{x_1, \dotsc, x_n, \dotsc}
    \qquad
    \begin{cases}
      p_i = \prob{\xi = x_i} > 0 \\
      \sum p_i = 1
    \end{cases}
  \end{equation*}
\end{definition}

Таким образом дискретная случайная величина задается законом распределения
(\(\sum p_i = 1\)):

\begin{ttable}{0.4 \linewidth}{X|X|X|X|X|X}  
  \(\xi\) & \(x_1\) & \(x_2\) & \(\dotsc\) & \(x_n\) & \(\dotsc\)
  \\ \hline
  \(p\)   & \(p_1\) & \(p_2\) & \(\dotsc\) & \(p_n\) & \(\dotsc\)
\end{ttable}

\begin{example} \label{ex:dice-dist-law}
  Бросаем кость, получаем следующий закон распределения:

  \begin{ttable}{0.4 \linewidth}{X|X|X|X|X|X|X}
    \(\xi\) & \(1\)   & \(2\)   & \(3\)   & \(4\)   & \(5\)   & \(6\)
    \\ \hline
    \(p\)   & \(1/6\) & \(1/6\) & \(1/6\) & \(1/6\) & \(1/6\) & \(1/6\)
  \end{ttable}
\end{example}

Все распределения из предыдущей лекции (биномиальное, геометрическое,
гипергеометрическое, Пуассона) являются дискретными.

\begin{example}
  Индикатор события \(A\).

  \begin{equation*}
    \xi(\omega) = \begin{cases}
      0, & \omega \notin A \text{~--- событие \(A\) произошло} \\
      1, & \omega \in A \text{~--- событие \(A\) не произошло}
    \end{cases}
  \end{equation*}
\end{example}

\subheader{Числовые характеристики дискретной случайной величины}

\subsubheader{I.}{Математическое ожидание (среднее значение)}

\begin{definition}
  Математическим ожиданием дискретной случайной величины \(\xi\) называется
  величина

  \begin{equation*}
    \expected{\xi} = \sum_{i = 1}^{\infty} x_i p_i
  \end{equation*}

  при условии, что данный ряд сходится абсолютно.
\end{definition}

\begin{remark}
  Если \(\sum_{i = 1}^{\infty} \abs{x_i} p_i = \infty\), то говорят, что
  математическое ожидание не существует.
\end{remark}

Смысл: математическое ожидание~--- значение, вокруг которого группируются все
остальные.

Статистический смысл: математическое ожидание~--- среднее арифметическое
наблюдаемых значений случайной величины при большом числе экспериментов.

\subsubheader{II.}{Дисперсия}

\begin{definition}
  Дисперсией случайной величины \(\xi\) называется среднее квадратов ее
  отклонения от математического ожидания.

  \begin{equation*}
    \variance{\xi}
    = \expected{\xi - \expected{\xi}}^2
    = \sum_{i = 1}^{\infty} \prh{x_i - \expected{\xi}}^2 p_i
  \end{equation*}

  При условии, что данный ряд сходится.
\end{definition}

\begin{remark}
  Дисперсию удобнее вычислять по формуле

  \begin{equation*}
    \variance{\xi}
    = \expected{\xi^2} - \prh{\expected{\xi}}^2
    = \sum_{i = 1}^{\infty} x_i^2 p_i - \prh{\expected{\xi}}^2
  \end{equation*}
\end{remark}

Смысл: квадрат среднего разброса рассеивания случайной величины относительно ее
математического ожидания.

\subsubheader{III.}{Среднее квадратическое отклонение}

\begin{definition}
  Средним квадратическим отклонением (СКО) называется величина

  \begin{equation*}
    \stder{\xi} = \sqrt{\variance{\xi}}
  \end{equation*}
\end{definition}

\begin{example}
  Бросание кости. Закон распределения описан в \ref{ex:dice-dist-law}.

  \begin{equation*}
    \begin{aligned}
      \expected{\xi}
      = \sum_{i = 1}^{6} x_i p_i
      = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6}
        + 4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6}
      = 3.5
    \\
      \variance{\xi}
      = \sum_{i = 1}^{6} x_i^2 p_i - \prh{\expected{\xi}}^2
      = 1 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 9 \cdot \frac{1}{6}
        + 16 \cdot \frac{1}{6} + 25 \cdot \frac{1}{6} + 36 \cdot \frac{1}{6}
        - 3.5^2
      \approx 2.92
    \\
      \stder{\xi}
      \approx \sqrt{2.92}
      \approx 1.71
    \end{aligned}
  \end{equation*}
\end{example}

\begin{example}
  Индикатор события \(A\) обычно обозначается \(I_A\). Опишем закон
  распределения случайной величины:

  \begin{ttable}{0.4 \linewidth}{X|X|X}
    \(I_A\) & \(0\)            & \(1\)
    \\ \hline
    \(p\)   & \(1 - \prob{A}\) & \(\prob{A}\)
  \end{ttable}

  Тогда

  \begin{equation*}
    \begin{aligned}
      \expected{\xi}
      = 0 \cdot \prh{1 - \prob{A}} + 1 \cdot \prob{A}
      = \prob{A}
    \\
      \variance{\xi}
      = 0^2 \cdot (1 - \prob{A}) + 1^2 \cdot \prob{A} - \prob{A}^2
      = \prob{A} \prh{1 - \prob{A}}
      = p q
    \end{aligned}
  \end{equation*}
\end{example}

\subheader{Свойства математического ожидания и дисперсии}

\begin{definition}
  Случайная величина \(\xi\) имеет вырожденное распределение, если

  \begin{equation*}
    \forall \omega \in \Omega \given \xi (\omega) = C
    \text{ или }
    \prob{\xi = C} = 1
  \end{equation*}

  Тогда

  \begin{equation*}
    \expected{\xi} = C
    \qquad
    \variance{\xi} = 0
  \end{equation*}
\end{definition}

\begin{lemma}
  \begin{equation*}
    \expected{\xi + C} = \expected{\xi} + C
    \qquad
    \variance{\xi + C} = \variance{\xi}
  \end{equation*}
\end{lemma}

\begin{proof}
  \begin{equation*}
    \begin{aligned}
      \expected{\xi + C}
      = \sum_{i = 1}^{\infty} (x_i + C) \cdot p_i
      = \sum_{i = 1}^{\infty} x_i p_i
        + C \cdot \under{\sum_{i = 1}^{\infty} p_i}{= 1}
      = \expected{\xi} + C
    \\
      \variance{\xi + C}
      = \expected{\xi + C - \expected{\xi + C}}^2
      = \expected{\xi + C - \expected{\xi} - C}^2
      = \expected{\xi - \expected{\xi}}^2
      = \variance{\xi}
    \end{aligned}
  \end{equation*}
\end{proof}

\begin{lemma}
  \begin{equation*}
    \expected{C \xi} = C \expected{\xi}
    \qquad
    \variance{C \xi} = C^2 \variance{\xi}
  \end{equation*}
\end{lemma}

\begin{proof}
  \begin{equation*}
    \begin{aligned}
      \expected{C \xi}
      = \sum_{i = 1}^{\infty} C x_i p_i
      = C \sum_{i = 1}^{\infty} x_i p_i
      = C \expected{\xi} 
    \\
      \variance{C \xi}
      = \expected{\prh{C \xi - \expected{C \xi}}^2}
      = \expected{\prh[\Big]{C \prh{\xi - \expected{\xi}}}^2}
      = C^2 \cdot \expected{\prh{\xi - \expected{\xi}}^2}
      = C^2 \variance{\xi} 
    \end{aligned}
  \end{equation*}
\end{proof}

\begin{lemma}
  \begin{equation*}
    \expected{\xi + \eta} = \expected{\xi} + \expected{\eta}
  \end{equation*}
\end{lemma}

\begin{proof}
  \begin{equation*}
    \expected{\xi + \eta}
    = \sum_{i, j = 1}^{\infty} \prh{x_i + y_j} \prob{\xi = x_i, \eta = y_j}
    = \sum_{i = 1}^{\infty} x_i
        \prh{\sum_{j = 1}^{\infty} \prob{\xi = x_i, \eta = y_j}}
      + \sum_{j = 1}^{\infty} y_i
        \prob{\sum_{i = 1}^{\infty} \prob{\xi = x_i, \eta = y_j}} 
  \end{equation*}

  Применим формулу полной вероятности к внутренним суммам.

  \begin{equation*}
    \expected{\xi + \eta}
    = \sum_{i = 1}^{\infty} x_i \prob{\xi = x_i}
      + \sum_{j = 1}^{\infty} y_i \prob{\eta = y_i}
    = \expected{\xi} + \expected{\eta}
  \end{equation*}
\end{proof}

\begin{definition}
  Случайные величины \(\xi\) и \(\eta\) называются независимыми, если

  \begin{equation*}
    \forall i, j \given
    \prob{\xi = x_i, \eta = y_j}
    = \prob{\xi = x_i} \cdot \prob{\eta = y_j}
  \end{equation*}

  т.е. случайные величины принимают свои значения независимо друг от друга.
\end{definition}

\begin{lemma} \label{lem:ind-expected}
  Если \(\xi\) и \(\eta\) независимы, то

  \begin{equation*}
    \expected{\xi \eta} = \expected{\xi} \expected{\eta}
  \end{equation*}
\end{lemma}

\begin{proof}
  \begin{equation*}
    \expected{\xi \eta}
    = \sum_{i, j = 1}^{\infty} (x_i y_j) \prob{\xi = x_i, \eta = y_j}
    = \sum_{i = 1}^{\infty} x_i
      \prh{\sum_{j = 1}^{\infty} y_j \prob{\xi = x_i, \eta = y_j}}
  \end{equation*}

  Т.к. события независимы, то по определению независимости имеем

  \begin{equation*}
    \expected{\xi \eta}
    = \sum_{i = 1}^{\infty} x_i \prob{\xi = x_i}
      \cdot \sum_{j = 1}^{\infty} y_j \prob{\eta = y_j}
    = \expected{\xi} \expected{\eta}
  \end{equation*}
\end{proof}

\begin{lemma}
  \begin{equation*}
    \variance{\xi} = \expected{\xi^2} - \prh{\expected{\xi} }^2
  \end{equation*}
\end{lemma}

\begin{proof}
  \begin{equation*}
    \variance{\xi}
    = \expected{\prh{\xi - \expected{\xi}}^2}
    = \expected{\xi^2 - 2 \xi \expected{\xi} + \prh{\expected{\xi}}^2}
    = \expected{\xi^2} - 2 \prh{\expected{\xi}}^2 + \prh{\expected{\xi}}^2
    = \expected{\xi^2} - \prh{\expected{\xi}}^2
  \end{equation*}
\end{proof}

\begin{lemma} \label{lem:variance-sum}
  \begin{equation*}
    \variance{\xi + \eta}
    = \variance{\xi} + \variance{\eta} + 2 \cov{\xi}{\eta}
    \qquad
    \cov{\xi}{\eta} = \expected{\xi \eta} - \expected{\xi} \expected{\eta}
  \end{equation*}
\end{lemma}

\begin{proof}
  \begin{equation*}
    \begin{aligned}
      \variance{\xi + \eta}
      & = \expected{(\xi + \eta)^2} - \prh{\expected{\xi + \eta}}^2
    \\
      & = \expected{\xi^2 + 2 \xi \eta + \eta^2}
        - \prh{\expected{\xi} + \expected{\eta}}^2
    \\
      & = \expected{\xi^2} + 2 \expected{\xi \eta} + \expected{\eta^2}
        - \prh{\expected{\xi}}^2 - 2 \expected{\xi} \expected{\eta}
        - \prh{\expected{\eta}}^2
    \\
      & = \prh[\bigg]{\expected{\xi^2} - \prh{\expected{\xi}}^2}
        + \prh[\bigg]{\expected{\eta^2} - \prh{\expected{\eta}}^2}
        + 2 \prh[\bigg]{\expected{\xi \eta} - \expected{\xi} \expected{\eta}}
    \\
      & = \variance{\xi} + \variance{\eta} + 2 \cov{\xi}{\eta}
    \end{aligned}
  \end{equation*}
\end{proof}

\begin{lemma}
  Если случайные величины \(\xi\) и \(\eta\) независимы, то

  \begin{equation*}
    \variance{\xi + \eta} = \variance{\xi} + \variance{\eta} 
  \end{equation*}
\end{lemma}

\begin{proof}
  Т.к. \(\xi\) и \(\eta\), то \(\cov{\xi}{\eta} = 0\) по \ref{lem:ind-expected}.
  Подставим это в \ref{lem:variance-sum} и получим искомое равенство.
\end{proof}

\subheader{Другие числовые характеристики}

\subsubheader{IV}{Моменты старших порядков}

\begin{definition}
  Момент \(k\)-ого порядка это

  \begin{equation*}
    m_k = \expected{\xi^k} 
  \end{equation*}
\end{definition}

\begin{definition}
  Центральный момент \(k\)-ого порядка это

  \begin{equation*}
    \mu_k = \expected{\prh{\xi - \expected{\xi}}^k} 
  \end{equation*}
\end{definition}

\begin{remark}
  \(\expected{\xi} = m_1\)~--- момент первого порядка. \(\variance{\xi} =
  \mu_2\)~--- центральный момент второго порядка.
\end{remark}

\begin{remark}
  Центральные моменты можно выразить через относительные.

  \begin{equation*}
    \begin{aligned}
      \mu_2
      = \variance{\xi}
      = \expected{\xi^2} - \prh{\expected{xi}}^2
      = m_2 - m_1^2
    \\
      \mu_3 = m_3 - 3 m_2 m_1 + 2 m_1^3
    \\
      \mu_4 = m_4 - 4 m_3 m_1 + 6 m_2 m_1^2 - 3 m_1^4
    \end{aligned}
  \end{equation*}
\end{remark}

\begin{remark}
  Задачу \ref{ex:prob-geom-2} можно решить, используя математическое ожидание.
  Пусть \(p = \prob{A}\)~--- вероятность того, что иголка пересекла стык, а
  случайная величина \(\xi\)~--- число пересечений стыка. Тогда, \(\xi\) это
  индикатор события \(A\), значит \(\expected{\xi} = p\).
  
  Используя свойства математического ожидания получаем, что если растянуть
  иголку в два раза, то математическое ожидание также возрастет вдвое. Растянем
  иголку в \(\pi\) раз и сделаем из нее окружность. Тогда математическое
  ожидание всегда будет равно двум вне зависимости от того, как эта окружность
  упадет на доску, значит

  \begin{equation*}
    \pi \cdot \expected{\xi} = 2
    \implies
    p = \frac{2}{\pi}
  \end{equation*}
\end{remark}